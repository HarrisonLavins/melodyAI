{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import seaborn as sb   # Heatmaps\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import util_plotting #For plotting loss over time\n",
    "\n",
    "import numpy as np\n",
    "from music21 import *\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "PICKLES_DIRECTORY = 'pickles'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading/Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{PICKLES_DIRECTORY}/int_short_seqs_pitch.pickle\", 'rb') as int_short_pitch:\n",
    "    pitch_seq = pickle.load(int_short_pitch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle the sequences so that transposed versions are not all adjacent\n",
    "# random.seed(0)\n",
    "# pitch_seq = random.sample(pitch_seq, len(pitch_seq))\n",
    "# pitch_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define and create vocab list of different pitches\n",
    "pitch_vocab = []\n",
    "\n",
    "pitches = itertools.chain(*pitch_seq)\n",
    "for p in pitches:\n",
    "    if p not in pitch_vocab:\n",
    "        pitch_vocab.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_vocab = sorted(pitch_vocab)\n",
    "print(\"There are\", len(pitch_vocab), \"unique characters in the dataset\")\n",
    "#pitch_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defines the dimension of the vocabulary of unique pitches\n",
    "vocab_size = len(pitch_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping from MIDI number/rest to unique index.\n",
    "# For example, to get the index of the midi pitch `69`, we can evaluate `char2idx[69]`.  \n",
    "# A rest `-1` would be evaluated by `char2idx[-1]`.\n",
    "midi2idx = {u:i for i, u in enumerate(pitch_vocab)}\n",
    "#midi2idx\n",
    "\n",
    "\n",
    "# Create a mapping from indices to midi pitches. This is\n",
    "#   the inverse of midi2idx and allows us to convert back\n",
    "#   from unique index to the midi pitch in our vocabulary.\n",
    "idx2midi = np.array(pitch_vocab)\n",
    "#idx2midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Converts the dataset MIDI pitches to their corresponding index values\n",
    "# ------------------------------------------------------------------------\n",
    "### Input: a 2D list of chorale MIDI pitches of dim.          (num_pieces X sequence_len)\n",
    "### Output: a 2D numpy array of chorale index numbers of dim. (num_pieces X sequence_len)\n",
    "def vectorize_seqs(pitch_seqs):\n",
    "    vectorized_pieces = []\n",
    "    for seq in pitch_seqs:\n",
    "        vectorized_seqs = []\n",
    "        for p in seq:\n",
    "            vectorized_seqs.append(midi2idx[p])\n",
    "        vectorized_pieces.append(vectorized_seqs)\n",
    "    \n",
    "    \n",
    "    return np.array(vectorized_pieces)\n",
    "\n",
    "vectorized_chorales = vectorize_seqs(pitch_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_chorales.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test that the mapping works and is consistent\n",
    "print('MIDI Pitch --> {:^15} --> idx2midi'.format('Vect. Index'))\n",
    "for i in range(10):\n",
    "    print(' {} {:^34} {}'.format(pitch_seq[0][i], vectorized_chorales[0][i], idx2midi[vectorized_chorales[0][i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training Examples and Targets\n",
    "Steps:\n",
    "- Pad the sequences to be all the same number of pitches long (seq_length+1)\n",
    "- Create and separate the training examples (x_train) from the targets (y_train)\n",
    "    - For each input, the corresponding target will contain the same length of pitches, except shifted one pitch to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pad the chorales with trailing 0's to make them all the same length\n",
    "padded_melodies = pad_sequences(vectorized_chorales, padding=\"post\", dtype='int32')\n",
    "#padded_melodies[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(padded_melodies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'All sequences are now {padded_melodies.shape[1]} pitches long.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Batch definition to return training examples X and targets Y ###\n",
    "# vectorized_melodies: np array of dim. (num_pieces, seq_length)\n",
    "# num_sequences: how many sequences (pieces) we want to return for use in training\n",
    "# seq_length:   the length of each sequence returned (all should be the same length due to zero-padding)\n",
    "# ------------------------------------------------------------------------------------\n",
    "def get_batch(vectorized_melodies, num_sequences, seq_length=padded_melodies.shape[1]):\n",
    "    # the length of the vectorized melodies (number of pieces to choose from) \n",
    "    n = vectorized_melodies.shape[0] - 1\n",
    "    #print(f'We have {n+1} melodies to choose from.')\n",
    "    # randomly choose the melodies for the examples in this training batch\n",
    "    mel_choice = np.random.choice(n, num_sequences)\n",
    "    #print(f'Melody sequence random choices for training: {mel_choice}')\n",
    "\n",
    "    # x_batch, y_batch provide the true inputs and targets for network training\n",
    "    '''construct a list of input sequences for the training batch'''\n",
    "    x_batch = np.array([vectorized_melodies[mel][0:-1] for mel in mel_choice])\n",
    "    '''construct a list of output sequences for the training batch (by shifting the input to the left)'''\n",
    "    y_batch = np.array([vectorized_melodies[mel][1:] for mel in mel_choice])\n",
    "\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_batch(padded_melodies, 1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Input sequence: ', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, Y = get_batch(padded_melodies, num_sequences=1)\n",
    "\n",
    "for i, (input_idx, target_idx) in enumerate(zip(np.squeeze(X), np.squeeze(Y))):\n",
    "    print(\"Timestep {:3d}\".format(i))\n",
    "    print(\"   input: {} midi-num: ({:s})\".format(input_idx, repr(idx2midi[input_idx])))\n",
    "    print(\"   expected output: {} ({:s})\".format(target_idx, repr(idx2midi[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the RNN Model\n",
    "\n",
    "The model is based off the LSTM architecture, where we use a state vector to maintain information about the temporal relationships between consecutive characters. <mark>The final output of the LSTM is then fed into a fully connected [`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layer where we'll output a softmax over each character in the vocabulary, and then sample from this distribution to predict the next character. </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM(rnn_units): \n",
    "  return tf.keras.layers.LSTM(\n",
    "    rnn_units, \n",
    "    return_sequences=True, \n",
    "    recurrent_initializer='glorot_uniform',\n",
    "    recurrent_activation='sigmoid'\n",
    "    #stateful=True,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, num_sequences):\n",
    "  model = tf.keras.Sequential([\n",
    "    # Layer 1: Embedding layer to transform indices into dense vectors of a fixed embedding size\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[num_sequences, None]),\n",
    "\n",
    "    # Layer 2: LSTM with `rnn_units` number of units. \n",
    "    LSTM(rnn_units),\n",
    "\n",
    "    # Layer 3: Dense (fully-connected) layer that transforms the LSTM output into the vocabulary size. \n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple model with default hyperparameters. You will get the chance to change these later.\n",
    "model = build_model(vocab_size, embedding_dim=256, rnn_units=1024, num_sequences=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_batch(padded_melodies, num_sequences=32)\n",
    "\n",
    "pred = model(X)\n",
    "print(\"Input shape:      \", X.shape, \" # (num_sequences, sequence_length)\")\n",
    "print(\"Prediction shape: \", pred.shape, \"# (num_sequences, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.numpy()[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the predictions from the model\n",
    "Using Seaborn heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,12))         # Sample figsize in inches\n",
    "heatmap = sb.heatmap(data, annot=True, linewidths=.5, ax=ax)\n",
    "plt.title('Network Output Distribution',fontsize=18)\n",
    "plt.xlabel('Softmax Layer at Timestep 0', fontsize=18)\n",
    "plt.ylabel('Pitch Prediction Indices', fontsize=18)\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pred.numpy()[0][0]\n",
    "print(data.shape)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pred.numpy()[0][0].reshape((44,1))\n",
    "print(data.shape)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function for Plotting Network Softmax as a Heatmap\n",
    "def plot_pred(pred, timestep):\n",
    "    data = pred.numpy()[0][0].reshape((44,1))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(3,12))         # Sample figsize in inches\n",
    "    heatmap = sb.heatmap(data, annot=True, linewidths=.5, ax=ax)\n",
    "    plt.title('Network Output Distribution',fontsize=18)\n",
    "    plt.xlabel('Softmax Layer at Timestep: {}'.format(timestep), fontsize=18)\n",
    "    plt.ylabel('Pitch Prediction Indices', fontsize=18)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions from the untrained model\n",
    "\n",
    "Let's take a look at what our untrained model is predicting.\n",
    "\n",
    "To get actual predictions from the model, we sample from the output distribution, which is defined by a `softmax` over our character vocabulary. This will give us actual character indices. This means we are using a [categorical distribution](https://en.wikipedia.org/wiki/Categorical_distribution) to sample over the example prediction. This gives a prediction of the next character (specifically its index) at each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(pred[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input (MIDI numbers): \\n\",idx2midi[X[0]])\n",
    "print()\n",
    "print(\"Untrained Next Pitch Predictions (MIDI numbers): \\n\", idx2midi[sampled_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model: loss and training operations\n",
    "\n",
    "Now it's time to train the model!\n",
    "\n",
    "At this point, we can think of our next character prediction problem as a standard classification problem. Given the previous state of the RNN, as well as the input at a given time step, we want to predict the class of the next character -- that is, to actually predict the next character. \n",
    "\n",
    "To train our model on this classification task, we can use a form of the `crossentropy` loss (negative log likelihood loss). Specifically, we will use the [`sparse_categorical_crossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/sparse_categorical_crossentropy) loss, as it utilizes integer targets for categorical classification tasks. We will want to compute the loss using the true targets -- the `labels` -- and the predicted targets -- the `logits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining the loss function ###\n",
    "\n",
    "'''Define the loss function to compute and return the loss between\n",
    "    the true labels and predictions (logits). Set the argument from_logits=True.'''\n",
    "def compute_loss(labels, logits):\n",
    "  loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "  return loss\n",
    "\n",
    "'''Compute the loss using the true next characters from the example batch \n",
    "    and the predictions from the untrained model several cells above'''\n",
    "example_batch_loss = compute_loss(Y, pred) \n",
    "\n",
    "print(\"Prediction shape: \", pred.shape, \" # (num_sequences, sequence_length, vocab_size)\") \n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter setting and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optimization parameters:\n",
    "num_training_iterations = 2000  # Increase this to train longer\n",
    "batch_size = 4  # defines the batch size (num_sequences) per training iteration: experiment between 1 and 64\n",
    "learning_rate = 5e-3  # Experiment between 1e-5 and 1e-1\n",
    "\n",
    "# Model parameters: \n",
    "#vocab_size = len(pitch_vocab) #already set above\n",
    "embedding_dim = 256 \n",
    "rnn_units = 1024  # Experiment between 1 and 2048\n",
    "\n",
    "# Checkpoint location: \n",
    "CHECKPOINT_DIR = './training_checkpoints'\n",
    "if not os.path.exists(CHECKPOINT_DIR):\n",
    "    os.makedirs(CHECKPOINT_DIR)\n",
    "checkpoint_prefix = os.path.join(CHECKPOINT_DIR, \"my_ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Define optimizer and training operation ###\n",
    "\n",
    "'''Instantiate a new model for training using the `build_model`\n",
    "  function and the hyperparameters created above.'''\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size)\n",
    "\n",
    "'''Instantiate an optimizer with its learning rate.\n",
    "  Checkout the tensorflow website for a list of supported optimizers.\n",
    "  https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/\n",
    "  Try using the Adam optimizer to start.'''\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y): \n",
    "  # Use tf.GradientTape()\n",
    "  with tf.GradientTape() as tape:\n",
    "  \n",
    "    '''TODO: feed the current input into the model and generate predictions'''\n",
    "    y_hat = model(x)\n",
    "  \n",
    "    '''TODO: compute the loss!'''\n",
    "    loss = compute_loss(y, y_hat)\n",
    "\n",
    "  # Now, compute the gradients \n",
    "  '''TODO: complete the function call for gradient computation. \n",
    "      Remember that we want the gradient of the loss with respect all \n",
    "      of the model parameters. \n",
    "      HINT: use `model.trainable_variables` to get a list of all model\n",
    "      parameters.'''\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "  \n",
    "  # Apply the gradients to the optimizer so it can update the model accordingly\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "  return loss\n",
    "\n",
    "##################\n",
    "# Begin training!#\n",
    "##################\n",
    "\n",
    "history = []\n",
    "plotter = util_plotting.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss')\n",
    "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
    "\n",
    "for iter in tqdm(range(num_training_iterations)):\n",
    "\n",
    "  # Grab a batch and propagate it through the network\n",
    "  x_batch, y_batch = get_batch(padded_melodies, batch_size)\n",
    "  loss = train_step(x_batch, y_batch)\n",
    "\n",
    "  # Update the progress bar\n",
    "  history.append(loss.numpy().mean())\n",
    "  plotter.plot(history)\n",
    "\n",
    "  # Update the model with the changed weights!\n",
    "  if iter % 100 == 0:     \n",
    "    model.save_weights(checkpoint_prefix)\n",
    "    \n",
    "# Save the trained model and the weights\n",
    "model.save_weights(checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Generate music using the RNN model\n",
    "\n",
    "Now, we can use our trained RNN model to generate some music! When generating music, we'll have to feed the model some sort of seed to get it started (because it can't predict anything without something to start with!).\n",
    "\n",
    "Once we have a generated seed, we can then iteratively predict each successive note (remember, we are using the MIDI representation for our music) using our trained RNN. More specifically, recall that our RNN outputs a `softmax` over possible successive notes. For inference, we iteratively sample from these distributions, and then use our samples to encode a generated piece using a Music21 stream in the MIDI format.\n",
    "\n",
    "Then, all we have to do is write it to a file and listen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore the latest checkpoint\n",
    "\n",
    "To keep this inference step simple, we will use a batch size of 1. Because of how the RNN state is passed from timestep to timestep, the model will only be able to accept a fixed batch size once it is built. \n",
    "\n",
    "To run the model with a different `batch_size`, we'll need to rebuild the model and restore the weights from the latest checkpoint, i.e., the weights after the last checkpoint during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TODO: Rebuild the model using a batch_size=1'''\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, num_sequences=1)\n",
    "\n",
    "# Restore the model weights for the last checkpoint after training\n",
    "model.load_weights(tf.train.latest_checkpoint(CHECKPOINT_DIR))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The prediction procedure\n",
    "\n",
    "Now, we're ready to write the code to generate music in the MIDI/Music21 format:\n",
    "\n",
    "* Initialize a \"seed\" start pitch and the RNN state, and set the number of pitches we want to generate.\n",
    "\n",
    "* Use the start pitch and the RNN state to obtain the probability distribution over the next predicted character.\n",
    "\n",
    "* Sample from multinomial distribution to calculate the index of the predicted pitch. This predicted pitch is then used as the next input to the model.\n",
    "\n",
    "* At each time step, the updated RNN state is fed back into the model, so that it now has more context in making the next prediction. After predicting the next character, the updated RNN states are again fed back into the model, which is how it learns sequence dependencies in the data, as it gets more information from the previous predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prediction of a generated song ###\n",
    "#Input:\n",
    "    # model: trained model with loaded training weights\n",
    "    # start_pitch: (int) a MIDI number representing the starting note for the sequence\n",
    "    # generation_length: (int) the desired length of the generated sequence\n",
    "#Output:\n",
    "    #A list of MIDI pitches of length `generation_length`+1 representing the predicted pitch sequence\n",
    "def generate_melody(model, start_pitch=69, generation_length=16):\n",
    "  # Evaluation step (generating pitch sequences using the learned RNN model)\n",
    "\n",
    "    '''Convert the start pitch to a number (vectorize)'''\n",
    "    input_eval = [midi2idx[start_pitch]]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Melody sequence list to store our results\n",
    "    melody_sequence = [start_pitch]\n",
    "\n",
    "  # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    tqdm._instances.clear()\n",
    "\n",
    "    for timestep in tqdm(range(generation_length)):\n",
    "        '''Evaluate the inputs and generate the next character predictions'''\n",
    "        predictions = model(input_eval)\n",
    "        \n",
    "        if timestep % 10 == 0:     \n",
    "            plot_pred(predictions, timestep)\n",
    "            #print('Predictions: ',predictions.shape)\n",
    "      \n",
    "        # Remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "      \n",
    "        '''Use a multinomial distribution to sample'''\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "      \n",
    "      # Pass the prediction along with the previous hidden state\n",
    "      #   as the next inputs to the model\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      \n",
    "        '''Add the predicted pitch to the generated sequence!'''\n",
    "        melody_sequence.append(idx2midi[predicted_id])\n",
    "        \n",
    "    \n",
    "    return melody_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the melodic sequence to actual music\n",
    "def melody2music(mel_sequence):\n",
    "    d = duration.Duration(1.0) #For now, the default duration is a quarter-note\n",
    "    \n",
    "    composed_stream = stream.Stream()\n",
    "    for curr_pitch in mel_sequence:\n",
    "        p = pitch.Pitch(curr_pitch)\n",
    "        n = note.Note()\n",
    "        n.pitch = p\n",
    "        n.duration = d\n",
    "        composed_stream.append(n)\n",
    "    \n",
    "    return composed_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compose Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Composition 1\n",
    "#######################################\n",
    "mel_sequence = generate_melody(model, 70)\n",
    "mel_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mel_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp1 = melody2music(mel_sequence)\n",
    "comp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp1.show('midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comp1.write('midi', 'first_comp.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Composition 2\n",
    "#######################################\n",
    "mel_sequence2 = generate_melody(model, 65)\n",
    "mel_sequence2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp2 = melody2music(mel_sequence2)\n",
    "comp2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comp2.show('midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For comparison - The master of his craft\n",
    "bachCorpus = corpus.getComposer('bach')\n",
    "pieceScore = corpus.parse(bachCorpus[69])\n",
    "melodyPart = pieceScore.getElementsByClass('Part')[0] \n",
    "melodyPart.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melodyPart.show('midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
